{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Red Translator\n",
    "### Complete pipeline on scraping foreign language Marxist texts, converting PDF scans to text files using Google's Tesseract OCR, and machine translating using DeepL\n",
    "\n",
    "---\n",
    "\n",
    "#### Python Library Installs"
   ],
   "id": "321d3701b456216"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:48:25.451802Z",
     "start_time": "2025-04-08T05:48:24.435220Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r requirements.txt",
   "id": "5ad804ccb4188b01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/tousif/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.32.3)\r\n",
      "Requirement already satisfied: bs4 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.0.2)\r\n",
      "Requirement already satisfied: pdf2image in /Users/tousif/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.17.0)\r\n",
      "Requirement already satisfied: pytesseract in /Users/tousif/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.3.13)\r\n",
      "Requirement already satisfied: deepl in /Users/tousif/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.21.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 1)) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 1)) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 1)) (2025.1.31)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from bs4->-r requirements.txt (line 2)) (4.13.3)\r\n",
      "Requirement already satisfied: pillow in /Users/tousif/miniconda3/lib/python3.12/site-packages (from pdf2image->-r requirements.txt (line 3)) (11.1.0)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from pytesseract->-r requirements.txt (line 4)) (24.2)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 2)) (2.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/tousif/miniconda3/lib/python3.12/site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 2)) (4.12.2)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "</div>\n",
    "\n",
    "â†’ Must have local download of Google Tesseract (Installation: [see here](https://tesseract-ocr.github.io/tessdoc/Installation.html)) and Poppler (Installation: [see here](https://pdf2image.readthedocs.io/en/latest/installation.html)) for the foreign language text extraction from PDF scans.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "</div>\n",
    "\n",
    "#### Imports"
   ],
   "id": "295577102637d564"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:48:33.260847Z",
     "start_time": "2025-04-08T05:48:25.478347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import deepl"
   ],
   "id": "ce41890bfd9583b9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "URL to extract PDF files from",
   "id": "3a525b9c38c3543a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:48:33.383985Z",
     "start_time": "2025-04-08T05:48:33.358155Z"
    }
   },
   "cell_type": "code",
   "source": "BASE_URL = \"http://www.bannedthought.net/China/Magazines/Hongqi\"",
   "id": "3e4d038b573bc858",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting up folders",
   "id": "b3415f58a9e8edec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:48:33.561680Z",
     "start_time": "2025-04-08T05:48:33.558476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DOWNLOAD_DIR = \"downloads\"\n",
    "TEXT_OUTPUT_DIR = \"data\"\n",
    "TRANSLATIONS_DIR = \"translations\"\n",
    "\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "os.makedirs(TEXT_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(TRANSLATIONS_DIR, exist_ok=True)"
   ],
   "id": "b1ab838998d5af2e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scrape PDF Files and Convert to Text",
   "id": "2d4a00210a9e8323"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:48:33.753509Z",
     "start_time": "2025-04-08T05:48:33.720345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_pdf_links(html_content, limit=None):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    pdf_links = []\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        if link[\"href\"].endswith(\".pdf\"):\n",
    "            pdf_links.append(link[\"href\"])\n",
    "        if limit and len(pdf_links) >= limit:\n",
    "            break\n",
    "    return pdf_links\n",
    "\n",
    "def download_pdf(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        filename = os.path.join(DOWNLOAD_DIR, os.path.basename(pdf_url))\n",
    "        with open(filename, \"wb\") as pdf_file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                pdf_file.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "        return filename\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading {pdf_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_pdf_to_images(pdf_path):\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        print(f\"Converted {pdf_path} to images.\")\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path} to images: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_text_from_images(images, output_filename):\n",
    "    try:\n",
    "        full_text = \"\"\n",
    "        for i, image in enumerate(images):\n",
    "            text = pytesseract.image_to_string(image, lang=\"chi_sim\")\n",
    "            full_text += text\n",
    "        output_path = os.path.join(TEXT_OUTPUT_DIR, output_filename)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(full_text)\n",
    "        print(f\"Extracted text saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "\n",
    "def main_pdf_to_text():\n",
    "    print(\"Starting PDF scraper...\")\n",
    "    html_content = fetch_page(BASE_URL)\n",
    "    if html_content:\n",
    "        pdf_links = parse_pdf_links(html_content, limit=3)\n",
    "        for pdf_link in pdf_links:\n",
    "            # Handle relative URLs\n",
    "            if not pdf_link.startswith(\"http\") and not pdf_link.startswith(\"https\"):\n",
    "                pdf_link = os.path.join(BASE_URL, pdf_link)\n",
    "            pdf_path = download_pdf(pdf_link)\n",
    "\n",
    "            if pdf_path:\n",
    "                images = convert_pdf_to_images(pdf_path)\n",
    "                if images:\n",
    "                    output_filename = os.path.splitext(os.path.basename(pdf_path))[0] + \".txt\"\n",
    "                    extract_text_from_images(images, output_filename)"
   ],
   "id": "9a64a76dda87a462",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:54:34.191323Z",
     "start_time": "2025-04-08T05:48:33.823328Z"
    }
   },
   "cell_type": "code",
   "source": "main_pdf_to_text()",
   "id": "24052a8ce47c2706",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PDF scraper...\n",
      "Downloaded: downloads/Hongqi1958N1.pdf\n",
      "Converted downloads/Hongqi1958N1.pdf to images.\n",
      "Extracted text saved to data/Hongqi1958N1.txt\n",
      "Downloaded: downloads/Hongqi1958N2.pdf\n",
      "Converted downloads/Hongqi1958N2.pdf to images.\n",
      "Extracted text saved to data/Hongqi1958N2.txt\n",
      "Downloaded: downloads/Hongqi1958N3.pdf\n",
      "Converted downloads/Hongqi1958N3.pdf to images.\n",
      "Extracted text saved to data/Hongqi1958N3.txt\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Translation",
   "id": "be2edf13bbed3c77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T06:02:05.624307Z",
     "start_time": "2025-04-08T06:02:05.620435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_text_into_chunks(text, max_chunk_size=5000):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for paragraph in text.split(\"\\n\\n\"):\n",
    "        if len(current_chunk) + len(paragraph) + 2 > max_chunk_size:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\"\n",
    "            current_chunk += paragraph\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def translate_text_files(input_dir, output_dir, target_language=\"EN-US\"):\n",
    "    DEEPL_API_KEY = os.environ.get(\"DEEPL_API_KEY\")\n",
    "    translator = deepl.Translator(DEEPL_API_KEY)\n",
    "\n",
    "    try:\n",
    "        for filename in os.listdir(input_dir):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                input_path = os.path.join(input_dir, filename)\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "\n",
    "                chunks = split_text_into_chunks(text)\n",
    "\n",
    "                translated_text = \"\"\n",
    "                for chunk in chunks:\n",
    "                    result = translator.translate_text(chunk, target_lang=target_language)\n",
    "                    translated_text += result.text + \"\\n\\n\"\n",
    "\n",
    "                output_filename = f\"translated_{filename}\"\n",
    "                output_path = os.path.join(output_dir, output_filename)\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(translated_text)\n",
    "\n",
    "                print(f\"Translated file saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text files: {e}\")"
   ],
   "id": "6e70718bb7fa9e65",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T06:05:13.166059Z",
     "start_time": "2025-04-08T06:02:11.817267Z"
    }
   },
   "cell_type": "code",
   "source": "translate_text_files(TEXT_OUTPUT_DIR, TRANSLATIONS_DIR, target_language=\"EN-US\")",
   "id": "f78eb8de95e2d1ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated file saved to translations/translated_Hongqi1958N2.txt\n",
      "Translated file saved to translations/translated_Hongqi1958N3.txt\n",
      "Translated file saved to translations/translated_Hongqi1958N1.txt\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
